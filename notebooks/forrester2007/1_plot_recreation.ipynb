{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = Path('../..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(str(module_path.resolve()))\n",
    "\n",
    "import mf2\n",
    "import multiLevelCoSurrogates as mlcs\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "\n",
    "np.random.seed(20160501)  # Setting seed for reproducibility\n",
    "OD = mf2.forrester\n",
    "\n",
    "np.set_printoptions(linewidth=200, edgeitems=10, precision=4, suppress=True)\n",
    "plot_dir = Path('../../plots/')\n",
    "data_dir = Path('../../files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Recreating the example plot in [Forrester2007 (Multi-fidelity optimization via surrogate modelling)](https://royalsocietypublishing.org/doi/full/10.1098/rspa.2007.1900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://royalsocietypublishing.org/cms/attachment/efa57e07-5384-4503-8b2b-ccbe632ffe87/3251fig1.jpg\" alt=\"Forrester2007 example plot\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = np.linspace(start=0,stop=1,num=501).reshape(-1,1)\n",
    "\n",
    "low_x = np.linspace(0,1,11).reshape(-1,1)\n",
    "high_x = low_x[[0,4,6,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high = OD.high(plot_x)\n",
    "plot_low = OD.low(plot_x)\n",
    "\n",
    "plt.plot(plot_x, plot_high, label='high')\n",
    "plt.plot(plot_x, plot_low, label='low')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the datapoints selected by the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_y = OD.high(high_x)\n",
    "low_y = OD.low(low_x)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-defining a default kernel *with* tunable hyperparameters\n",
    "kernel = kernels.ConstantKernel(constant_value=1.0) \\\n",
    "            * kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Gaussian Process models for each fidelity exclusively. Low-fidelity is a good fit, high fidelity is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_direct = GaussianProcessRegressor(kernel=kernel)\n",
    "gp_direct.fit(high_x, high_y)\n",
    "\n",
    "gp_low = GaussianProcessRegressor(kernel=kernel)\n",
    "gp_low.fit(low_x, low_y)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-Kriging formulation is $\\hat{f}_h(x) = \\rho * f_l(x) + \\delta(x)$. <br>\n",
    "$\\hat{f}_h(x)$ is the high-fidelity prediction at $x$<br>\n",
    "$\\rho$ is a scaling factor<br>\n",
    "$f_l(x)$ is a low-fidelity information input (either actual or another model) at $x$<br>\n",
    "$\\delta(x)$ is a prediction for the difference between $f_h(x)$ and $\\rho * f_l(x)$<br>\n",
    "\n",
    "$\\rho$ is calculated as $1 / (1/n)\\Sigma_{i=1}^n f_h(x_i) / f_l(x_i)$, i.e. `1/mean(f_high(x_high) / f_low(x_high))` with `x_high` being all input for which we have high-fidelity outcomes.\n",
    "\n",
    "Here we start by plotting just the parts of this equation.<br>\n",
    "In this example, there is an explicit scaling factor of __2__ between high and low fidelity that is seen to be easily captured by the difference model $\\delta(x)$, i.e. `gp_diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_at_high = np.array(OD.low([x for x in high_x])).reshape(-1,1)\n",
    "scale = 1/np.mean(high_y / low_at_high)\n",
    "\n",
    "diff_x = high_x\n",
    "diff_y = np.array([(OD.high(x) - scale*OD.low(x)) for x in diff_x])\n",
    "gp_diff = GaussianProcessRegressor(kernel=kernel)\n",
    "gp_diff.fit(diff_x, diff_y)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, plot_high - plot_low, label='diff')\n",
    "plt.plot(plot_x, gp_diff.predict(plot_x), label='scaled diff-fit GP')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale` parameter here is an estimate based on the datapoints we have. For this example with only four high-fidelity points, this is a reasonable, but not exact fit. The actual value according to the function definition should be 2, and the value stated by the paper to match best in the x-range [0,1] is 1.87."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with the actual co-kriging prediction plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_y = lambda x: scale*gp_low.predict(x) + gp_diff.predict(x)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, co_y(plot_x), label='co-kriging')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct construction with (Hierarchical)Surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the same plot as above using our own (Hierarchical)Surrogate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive only has to be created once...\n",
    "archive = mlcs.CandidateArchive(ndim=1, fidelities=['high', 'low', 'high-low'])\n",
    "archive.addcandidates(low_x, low_y, fidelity='low')\n",
    "archive.addcandidates(high_x, high_y, fidelity='high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without normalization by Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surr_high = mlcs.Surrogate.fromname('Kriging', archive, fidelity='high', normalized=False)\n",
    "surr_low = mlcs.Surrogate.fromname('Kriging', archive, fidelity='low', normalized=False)\n",
    "surr_hier = mlcs.HierarchicalSurrogate('Kriging', surr_low, archive, ['high', 'low'], normalized=False)\n",
    "\n",
    "surr_high.train()\n",
    "surr_low.train()\n",
    "surr_hier.train()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(plot_x, OD.high(plot_x), label='high')\n",
    "plt.plot(plot_x, OD.low(plot_x), label='low')\n",
    "plt.plot(plot_x, surr_high.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, surr_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, surr_hier.predict(plot_x), label='co-kriging')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With normalization by Surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to show that the normalization is correctly implemented.<br>\n",
    "Because of the values in this example, it's not really needed, but if the results at least don't get worse in this case, it's probably correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surr_high = mlcs.Surrogate.fromname('Kriging', archive, fidelity='high', normalized=True)\n",
    "surr_low = mlcs.Surrogate.fromname('Kriging', archive, fidelity='low', normalized=True)\n",
    "surr_hier = mlcs.HierarchicalSurrogate('Kriging', surr_low, archive, ['high', 'low'], normalized=True)\n",
    "\n",
    "surr_high.train()\n",
    "surr_low.train()\n",
    "surr_hier.train()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(plot_x, OD.high(plot_x), label='high')\n",
    "plt.plot(plot_x, OD.low(plot_x), label='low')\n",
    "plt.plot(plot_x, surr_high.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, surr_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, surr_hier.predict(plot_x), label='co-kriging')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct construction with MultiFidelityBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the same plot again with the MultiFidelityBO (Bayesian Optimization) interface.<br>\n",
    "This interface automatically creates a full set of hierarchical models for any number of fidelities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfbo = mlcs.MultiFidelityBO(OD, archive)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(plot_x, OD.high(plot_x), label='high')\n",
    "plt.plot(plot_x, OD.low(plot_x), label='low')\n",
    "plt.plot(plot_x, mfbo.direct_models['high'].predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, mfbo.models['low'].predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, mfbo.models['high'].predict(plot_x), label='co-kriging')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}forrester2007_recreated.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the match exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two changes to the procedure to really recreate the plot:\n",
    " 1. Using $f_l(x)$ directly rather than model $\\hat{f}_l(x)$\n",
    " 2. Using better scaling values. `1.87` gives the match seen in the original picture, while `2` gives a perfect match \n",
    "\n",
    "The first change should actually be used too. If predicting some $\\hat{f}_h(x)$ value for a completely new point $x$, then obviously the lower-fidelity models are the only available source of information. But when selecting which point to evaluate in higher fidelity, the exact lower fidelity information is usually available and can therefore be used.\n",
    "\n",
    "The value `1.87` comes from taking the mean over the entire range (based on 100 samples) rather than just the 4 common datapoints we have, while the value `2` is derived from the function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   n  | Mean ratio f_h / f_l')\n",
    "print('------+----------------------')\n",
    "for n in [50, 100, 200, 300, 400, 500, 750, 1000]:\n",
    "    high = OD.high(np.linspace(0,1,n+1))\n",
    "    low = OD.low(np.linspace(0,1,n+1))\n",
    "    rho = 1/np.mean(high/low)\n",
    "    print(f'{n:>5} |       {rho:<.6}  {\"<---\" if n==100 else \"\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_diff_20 = GaussianProcessRegressor(kernel=kernel).fit(diff_x, np.array([(OD.high(x) - 2*OD.low(x)) for x in diff_x]))\n",
    "gp_diff_187 = GaussianProcessRegressor(kernel=kernel).fit(diff_x, np.array([(OD.high(x) - 1.87*OD.low(x)) for x in diff_x]))\n",
    "\n",
    "cokriging_y_20 = lambda x: 2*OD.low(x) + gp_diff_20.predict(x)\n",
    "cokriging_y_187 = lambda x: 1.87*OD.low(x) + gp_diff_187.predict(x)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, cokriging_y_20(plot_x), label='co-kriging (2)')\n",
    "plt.plot(plot_x, cokriging_y_187(plot_x), label='co-kriging (1.87)')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'{plot_dir}accurate_forrester2007.png')\n",
    "plt.savefig(f'{plot_dir}accurate_forrester2007.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side by side comparison\n",
    "<img src=\"https://royalsocietypublishing.org/cms/attachment/efa57e07-5384-4503-8b2b-ccbe632ffe87/3251fig1.jpg\" alt=\"Forrester2007 example plot\" width=\"362\"/><img src=\"../../plots/accurate_forrester2007.png\" alt=\"Recreated Forrester2007 example plot\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
