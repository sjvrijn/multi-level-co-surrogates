{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable   \n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from itertools import product\n",
    "from more_itertools import chunked\n",
    "from functools import partial\n",
    "import multifidelityfunctions as mff\n",
    "import multiLevelCoSurrogates as mlcs\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(20160501)  # Setting seed for reproducibility\n",
    "OD = mff.oneDimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print settings/helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import clear_output\n",
    "from pprint import pprint\n",
    "np.set_printoptions(linewidth=200)\n",
    "plot_dir = '../multiLevelCoSurrogates/plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Recreating the example plot in [Forrester2007 (Multi-fidelity optimization via surrogate modelling)](https://royalsocietypublishing.org/doi/full/10.1098/rspa.2007.1900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://royalsocietypublishing.org/cms/attachment/efa57e07-5384-4503-8b2b-ccbe632ffe87/3251fig1.jpg\" alt=\"Forrester2007 example plot\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = np.linspace(start=0,stop=1,num=501).reshape(-1,1)\n",
    "\n",
    "low_x = np.linspace(0,1,11).reshape(-1,1)\n",
    "high_x = low_x[[0,4,6,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high = OD.high(plot_x)\n",
    "plot_low = OD.low(plot_x)\n",
    "\n",
    "plt.plot(plot_x, plot_high, label='high')\n",
    "plt.plot(plot_x, plot_low, label='low')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the datapoints selected by the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_y = OD.high(high_x)\n",
    "low_y = OD.low(low_x)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-defining a default kernel *with* tunable hyperparameters\n",
    "kernel = kernels.ConstantKernel(constant_value=1.0) \\\n",
    "            * kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Gaussian Process models for each fidelity exclusively. Low-fidelity is a good fit, high fidelity is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_direct = GaussianProcessRegressor(kernel=kernel)\n",
    "gp_direct.fit(high_x, high_y)\n",
    "\n",
    "gp_low = GaussianProcessRegressor(kernel=kernel)\n",
    "gp_low.fit(low_x, low_y)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-Kriging formulation is $\\hat{f}_h(x) = \\rho * f_l(x) + \\delta(x)$. <br>\n",
    "$\\hat{f}_h(x)$ is the high-fidelity prediction at $x$<br>\n",
    "$\\rho$ is a scaling factor<br>\n",
    "$f_l(x)$ is a low-fidelity information input (either actual or another model) at $x$<br>\n",
    "$\\delta(x)$ is a prediction for the difference between $f_h(x)$ and $\\rho * f_l(x)$<br>\n",
    "\n",
    "$\\rho$ is calculated as $1 / (1/n)\\Sigma_{i=1}^n f_h(x_i) / f_l(x_i)$, i.e. `1/mean(f_high(x_high) / f_low(x_high))` with `x_high` being all input for which we have high-fidelity outcomes.\n",
    "\n",
    "Here we start by plotting just the parts of this equation.<br>\n",
    "In this example, there is an explicit scaling factor of __2__ between high and low fidelity that is seen to be easily captured by the difference model $\\delta(x)$, i.e. `gp_diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_at_high = np.array(OD.low([x for x in high_x])).reshape(-1,1)\n",
    "scale = 1/np.mean(high_y / low_at_high)\n",
    "\n",
    "diff_x = high_x\n",
    "diff_y = np.array([(OD.high(x) - scale*OD.low(x)) for x in diff_x])\n",
    "gp_diff = GaussianProcessRegressor(kernel=kernel)\n",
    "gp_diff.fit(diff_x, diff_y)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, plot_high - plot_low, label='diff')\n",
    "plt.plot(plot_x, gp_diff.predict(plot_x), label='scaled diff-fit GP')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale` parameter here is an estimate based on the datapoints we have. For this example with only four high-fidelity points, this is a reasonable, but not exact fit. The actual value according to the function definition should be 2, and the value stated by the paper to match best in the x-range [0,1] is 1.87."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with the actual co-kriging prediction plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_y = lambda x: scale*gp_low.predict(x) + gp_diff.predict(x)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, co_y(plot_x), label='co-kriging')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct construction with (Hierarchical)Surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the same plot as above using our own (Hierarchical)Surrogate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive only has to be created once...\n",
    "archive = mlcs.CandidateArchive(ndim=1, fidelities=['high', 'low', 'high-low'])\n",
    "archive.addcandidates(low_x, low_y, fidelity='low')\n",
    "archive.addcandidates(high_x, high_y, fidelity='high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without normalization by Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surr_high = mlcs.Surrogate.fromname('Kriging', archive, fidelity='high', normalized=False)\n",
    "surr_low = mlcs.Surrogate.fromname('Kriging', archive, fidelity='low', normalized=False)\n",
    "surr_hier = mlcs.HierarchicalSurrogate('Kriging', surr_low, archive, ['high', 'low'], normalized=False)\n",
    "\n",
    "surr_high.train()\n",
    "surr_low.train()\n",
    "surr_hier.train()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(plot_x, OD.high(plot_x), label='high')\n",
    "plt.plot(plot_x, OD.low(plot_x), label='low')\n",
    "plt.plot(plot_x, surr_high.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, surr_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, surr_hier.predict(plot_x), label='co-kriging')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With normalization by Surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to show that the normalization is correctly implemented.<br>\n",
    "Because of the values in this example, it's not really needed, but if the results at least don't get worse in this case, it's probably correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surr_high = mlcs.Surrogate.fromname('Kriging', archive, fidelity='high', normalized=True)\n",
    "surr_low = mlcs.Surrogate.fromname('Kriging', archive, fidelity='low', normalized=True)\n",
    "surr_hier = mlcs.HierarchicalSurrogate('Kriging', surr_low, archive, ['high', 'low'], normalized=True)\n",
    "\n",
    "surr_high.train()\n",
    "surr_low.train()\n",
    "surr_hier.train()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(plot_x, OD.high(plot_x), label='high')\n",
    "plt.plot(plot_x, OD.low(plot_x), label='low')\n",
    "plt.plot(plot_x, surr_high.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, surr_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, surr_hier.predict(plot_x), label='co-kriging')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct construction with MultiFidelityBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the same plot again with the MultiFidelityBO (Bayesian Optimization) interface.<br>\n",
    "This interface automatically creates a full set of hierarchical models for any number of fidelities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfbo = mlcs.MultiFidelityBO(OD, archive, output_range=(-10, 16))\n",
    "\n",
    "# Plotting\n",
    "plt.plot(plot_x, OD.high(plot_x), label='high')\n",
    "plt.plot(plot_x, OD.low(plot_x), label='low')\n",
    "plt.plot(plot_x, mfbo.direct_models['high'].predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, mfbo.models['low'].predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, mfbo.models['high'].predict(plot_x), label='co-kriging')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}forrester2007_recreated.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the match exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two changes to the procedure to really recreate the plot:\n",
    " 1. Using $f_l(x)$ directly rather than model $\\hat{f}_l(x)$\n",
    " 2. Using better scaling values. `1.87` gives the match seen in the original picture, while `2` gives a perfect match \n",
    "\n",
    "The first change should actually be used too. If predicting some $\\hat{f}_h(x)$ value for a completely new point $x$, then obviously the lower-fidelity models are the only available source of information. But when selecting which point to evaluate in higher fidelity, the exact lower fidelity information is usually available and can therefore be used.\n",
    "\n",
    "The value `1.87` comes from taking the mean over the entire range rather than just the 4 common datapoints we have, while the value `2` is derived from the function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/np.mean(plot_high/plot_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_diff_20 = GaussianProcessRegressor(kernel=kernel).fit(diff_x, np.array([(OD.high(x) - 2*OD.low(x)) for x in diff_x]))\n",
    "gp_diff_187 = GaussianProcessRegressor(kernel=kernel).fit(diff_x, np.array([(OD.high(x) - 1.87*OD.low(x)) for x in diff_x]))\n",
    "\n",
    "cokriging_y_20 = lambda x: 2*OD.low(x) + gp_diff_20.predict(x)\n",
    "cokriging_y_187 = lambda x: 1.87*OD.low(x) + gp_diff_187.predict(x)\n",
    "\n",
    "line, = plt.plot(plot_x, plot_high, label='high')\n",
    "plt.scatter(high_x, high_y, color=line.get_color())\n",
    "line, = plt.plot(plot_x, plot_low, label='low')\n",
    "plt.scatter(low_x, low_y, color=line.get_color())\n",
    "plt.plot(plot_x, gp_direct.predict(plot_x), label='high-fit GP')\n",
    "plt.plot(plot_x, gp_low.predict(plot_x), label='low-fit GP')\n",
    "plt.plot(plot_x, cokriging_y_20(plot_x), label='co-kriging (2)')\n",
    "plt.plot(plot_x, cokriging_y_187(plot_x), label='co-kriging (1.87)')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'{plot_dir}accurate_forrester2007.png')\n",
    "plt.savefig(f'{plot_dir}accurate_forrester2007.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side by side comparison\n",
    "<img src=\"https://royalsocietypublishing.org/cms/attachment/efa57e07-5384-4503-8b2b-ccbe632ffe87/3251fig1.jpg\" alt=\"Forrester2007 example plot\" width=\"362\"/><img src=\"../multiLevelCoSurrogates/plots/accurate_forrester2007.png\" alt=\"Recreated Forrester2007 example plot\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Trade-off heatmap: number of high- vs. low-fidelity points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers an experiment about the influence of low-fidelity points is in a co-surrogate setup.\n",
    "\n",
    "Let $n_L$ be the number of low-fidelity points and $n_H$ the number of high-fidelity points. Create a sample $x_L$ of $n_L$ points using some initial sampling method (random, LHS, grid, etc), and take from that a subsample $x_H \\subset x_L$ through some heuristic (maximal distance, random, etc). Then we train a number of models:\n",
    " - direct low-fidelity model using $x_L, f_L(x_L)$ only\n",
    " - direct high-fidelity model using $x_H, f_H(x_H)$ only\n",
    " - hierarchical high-fidelity model using both $x_L, f_L(x_L)$ and $x_H, f_H(x_H)$\n",
    " \n",
    "Independently, a function-dependent sample $x_{mse}$ of size 1000 is also created. This sample is used to calculate a Mean Squared Error (MSE) value for the state of a model after training.\n",
    "\n",
    "For the experiments, we examine all combinations for $n_L \\in 3, \\ldots, 100$ and $n_H \\in 2, \\ldots, 40$, with the restriction that $n_L > n_H$. Each combination is repeated 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_high = 40\n",
    "max_low = 100\n",
    "num_reps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_random_sample(ndim, nlow):\n",
    "    return np.random.rand(nlow, ndim)\n",
    "\n",
    "def low_lhs_sample(ndim, nlow):\n",
    "    if ndim == 1:\n",
    "        return np.linspace(0,1,nlow).reshape(-1,1)\n",
    "    elif ndim > 1:\n",
    "        return lhs(ndim, nlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mse_tracking(func, sample_generator, \n",
    "                        max_high=40, max_low=100, num_reps=30,\n",
    "                        min_high=2, min_low=3):\n",
    "    ndim = func.ndim\n",
    "    mse_tracking = np.empty((max_high+1, max_low+1, num_reps, 3))\n",
    "    mse_tracking[:] = np.nan\n",
    "    cases = list(product(range(min_high, max_high+1), range(min_low, max_low+1), range(num_reps)))\n",
    "\n",
    "    for idx, case in enumerate(cases):\n",
    "        num_high, num_low, rep = case\n",
    "\n",
    "        if num_high >= num_low:\n",
    "            continue\n",
    "        if idx % 100 == 0:\n",
    "            clear_output()\n",
    "            print(f'{idx}/{len(cases)}')\n",
    "\n",
    "        low_x = sample_generator(ndim, num_low)\n",
    "        high_x = low_x[np.random.choice(num_low, num_high, replace=False)]\n",
    "        \n",
    "        archive = mlcs.CandidateArchive(ndim=ndim, fidelities=['high', 'low', 'high-low'])\n",
    "        archive.addcandidates(low_x, func.low(low_x), fidelity='low')\n",
    "        archive.addcandidates(high_x, func.high(high_x), fidelity='high')\n",
    "\n",
    "        mfbo = mlcs.MultiFidelityBO(func, archive, output_range=(-10, 16))\n",
    "        mse_tracking[num_high, num_low, rep] = mfbo.getMSE()\n",
    "\n",
    "    clear_output()\n",
    "    print(f'{len(cases)}/{len(cases)}')\n",
    "    return mse_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_high_vs_low_num_samples(data, name, vmin=.5, vmax=100):\n",
    "    norm = colors.LogNorm(vmin=vmin, vmax=vmax, clip=True)\n",
    "    fig, ax = plt.subplots(figsize=(9,3.5))\n",
    "    \n",
    "    ax.set_aspect(1.)\n",
    "    \n",
    "    plt.title('high (hierarchical)')\n",
    "    img = ax.imshow(data[:,:,0], cmap='viridis_r', norm=norm)\n",
    "    \n",
    "    divider = make_axes_locatable(ax)\n",
    "    axx = divider.append_axes(\"bottom\", size=.2, pad=0.05, sharex=ax)\n",
    "    axy = divider.append_axes(\"left\", size=.2, pad=0.05, sharey=ax)\n",
    "    \n",
    "    ax.xaxis.set_tick_params(labelbottom=False)\n",
    "    ax.yaxis.set_tick_params(labelleft=False)\n",
    "    axy.xaxis.set_tick_params(labelbottom=False)\n",
    "    axx.yaxis.set_tick_params(labelleft=False)\n",
    "    \n",
    "    img = axy.imshow(np.nanmean(data[:,:,1], axis=1).reshape(-1,1), cmap='viridis_r', norm=norm)\n",
    "    img = axx.imshow(np.nanmean(data[:,:,2], axis=0).reshape(1,-1), cmap='viridis_r', norm=norm)\n",
    "    \n",
    "    fig.colorbar(img, ax=ax, orientation='vertical')\n",
    "    axy.set_ylabel('#High-fid samples')\n",
    "    axx.set_xlabel('#Low-fid samples')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plot_dir}{name}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mse_tracking.npy' in os.listdir('.'):\n",
    "    mse_tracking = np.load('mse_tracking.npy')\n",
    "else:\n",
    "    mse_tracking = create_mse_tracking(OD, low_random_sample)\n",
    "    np.save('mse_tracking.npy', mse_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('median')\n",
    "pprint([(f'{95+i}%-ile', np.percentile(np.nanmedian(mse_tracking, axis=2).flatten(), 95+i)) for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high_vs_low_num_samples(np.nanmedian(mse_tracking, axis=2), 'high-low-samples-random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linspace, random subsample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'lin_mse_tracking.npy' in os.listdir('.'):\n",
    "    lin_mse_tracking = np.load('lin_mse_tracking.npy')\n",
    "else:\n",
    "    lin_mse_tracking = create_mse_tracking(OD, low_lhs_sample)\n",
    "    np.save('lin_mse_tracking.npy', lin_mse_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('median')\n",
    "pprint([(f'{95+i}%-ile', np.percentile(np.nanmedian(lin_mse_tracking, axis=2).flatten(), 95+i)) for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high_vs_low_num_samples(np.nanmedian(lin_mse_tracking, axis=2), 'high-low-samples-linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    " - [x] high (direct) and low (direct) plots have no reason to be a 2d heatmap: only one value has any impact anyway. Any other effect is simply through random sampling\n",
    " - [ ] For linear low-fidelity sampling: consider changing random to exhaustive sub-sample sets for small values of $n_L$ and $n_H$\n",
    " - [ ] Plot MSE difference between high (direct) and high (hierarchical) to show accuracy difference\n",
    " - [x] Investigate factor-of-eight vertical lines of bad performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code-cell + output is no longer required..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 1\n",
    "num_low = 17\n",
    "num_high = 10\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(16, 14))\n",
    "for ax in axes.flatten():\n",
    "    low_x = low_lhs_sample(ndim, num_low)\n",
    "    high_x = low_x[np.random.choice(num_low, num_high, replace=False)]\n",
    "\n",
    "    archive = mlcs.CandidateArchive(ndim=ndim, fidelities=['high', 'low', 'high-low'])\n",
    "    archive.addcandidates(low_x, OD.low(low_x), fidelity='low')\n",
    "    archive.addcandidates(high_x, OD.high(high_x), fidelity='high')\n",
    "\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        mfbo = mlcs.MultiFidelityBO(OD, archive, output_range=(-10, 16))\n",
    "    \n",
    "    line, = ax.plot(plot_x, plot_high, label='high')\n",
    "    ax.scatter(high_x, OD.high(high_x), color=line.get_color())\n",
    "    line, = ax.plot(plot_x, plot_low, label='low')\n",
    "    ax.scatter(low_x, OD.low(low_x), color=line.get_color())\n",
    "    ax.plot(plot_x, mfbo.models['high'].predict(plot_x), label='co-kriging')\n",
    "    ax.set_title(mfbo.getMSE()[0])\n",
    "    ax.set_ylim([-10, 20])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}factor-eight-examples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALT_create_mse_tracking(func, sample_generator, \n",
    "                            max_high=40, max_low=100, num_reps=30,\n",
    "                            min_high=2, min_low=3):\n",
    "    ndim = func.ndim\n",
    "    mse_tracking = np.empty((max_high+1, max_low+1, num_reps, 3))\n",
    "    mse_tracking[:] = np.nan\n",
    "    cases = list(product(range(min_high, max_high+1), range(min_low, max_low+1), range(num_reps)))\n",
    "\n",
    "    for idx, case in enumerate(cases):\n",
    "        num_high, num_low, rep = case\n",
    "\n",
    "        if num_high >= num_low:\n",
    "            continue\n",
    "        if idx % 100 == 0:\n",
    "            clear_output()\n",
    "            print(f'{idx}/{len(cases)}')\n",
    "\n",
    "        low_x = sample_generator(ndim, num_low)\n",
    "        high_x = low_x[np.random.choice(num_low, num_high, replace=False)]\n",
    "        \n",
    "        archive = mlcs.CandidateArchive(ndim=ndim, fidelities=['high', 'low', 'high-low'])\n",
    "        archive.addcandidates(low_x, func.low(low_x), fidelity='low')\n",
    "        archive.addcandidates(high_x, func.high(high_x), fidelity='high')\n",
    "\n",
    "        mfbo = mlcs.MultiFidelityBO(func, archive, output_range=(-10, 16))\n",
    "        mse_tracking[num_high, num_low, rep] = mfbo.getMSE()\n",
    "        \n",
    "        if num_high > 15 and mse_tracking[num_high, num_low, rep, 0] > 1:\n",
    "            with open('occurrences_output.txt', 'a') as f:\n",
    "                f.write(*case)\n",
    "                f.write(low_x.flatten().tolist())\n",
    "                f.write(high_x.flatten().tolist())\n",
    "                f.write()\n",
    "\n",
    "    clear_output()\n",
    "    print(f'{len(cases)}/{len(cases)}')\n",
    "    return mse_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'ALT_mse_tracking.npy' in os.listdir('.'):\n",
    "#     alt_mse_tracking = np.load('ALT_mse_tracking.npy')\n",
    "# else:\n",
    "#     alt_mse_tracking = ALT_create_mse_tracking(OD, low_lhs_sample,\n",
    "#                                                min_high=15, max_high=40,\n",
    "#                                                min_low=16, max_low=60,\n",
    "#                                                num_reps=20)\n",
    "#     np.save('ALT_mse_tracking.npy', lin_mse_tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGO - 1D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First creating an inverted function as BO is currently hardcoded for maximization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_OD = mff.MultiFidelityFunction(\n",
    "    u_bound=np.array(OD.u_bound), l_bound=np.array(OD.l_bound),\n",
    "    functions=[lambda x: -OD.high(x), lambda x: -OD.low(x)],\n",
    "    fidelity_names=['high', 'low'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_x = np.linspace(0,1,6).reshape((-1,1))\n",
    "high_x = low_x[[2,3]].reshape((-1,1))\n",
    "\n",
    "archive = mlcs.CandidateArchive(ndim=1, fidelities=['high', 'low', 'high-low'])\n",
    "archive.addcandidates(low_x, inv_OD.low(low_x), fidelity='low')\n",
    "archive.addcandidates(high_x, inv_OD.high(high_x), fidelity='high')\n",
    "\n",
    "np.random.seed(20160501)\n",
    "mfbo = mlcs.MultiFidelityBO(inv_OD, archive, output_range=(-16, 10), schema=[1,1])\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(3,3, figsize=(12,9))\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        plot_hier, std_hier = mfbo.models['high'].predict(plot_x, mode='both')\n",
    "    \n",
    "    line_1, = ax.plot(plot_x, inv_OD.high(plot_x), label='high')\n",
    "    line_2, = ax.plot(plot_x, inv_OD.low(plot_x), label='low')\n",
    "    line_high, = ax.plot(plot_x, mfbo.direct_models['high'].predict(plot_x), label='high-fit GP')\n",
    "    line_hier, = ax.plot(plot_x, mfbo.models['high'].predict(plot_x), label='co-kriging')\n",
    "    scat_2 = ax.scatter(*archive.getcandidates(fidelity='low'), color=line_2.get_color())\n",
    "    scat_1 = ax.scatter(*archive.getcandidates(fidelity='high'), color=line_1.get_color())\n",
    "    \n",
    "    ax.fill_between(plot_x.flatten(), plot_hier - 3*std_hier, plot_hier + 3*std_hier, alpha=.25, color=line_hier.get_color())\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        line_acq, = ax2.plot(plot_x, mfbo.utility(plot_x, gp=mfbo.models['high'], y_max=archive.max['high']),\n",
    "                             alpha=.5, label='acq', color='C4')\n",
    "        line_std_low, = ax2.plot(plot_x, mfbo.models['low'].predict(plot_x, mode='std'),\n",
    "                                 alpha=.5, label='std low', color='C5', ls='--', )\n",
    "        line_std_diff, = ax2.plot(plot_x, mfbo.models['high'].diff_model.predict(plot_x, mode='std'),\n",
    "                                  alpha=.5, label='std diff', color='C6', ls=':', )\n",
    "#     line_std, = ax2.plot(plot_x, std_hier, color='C7', label='std hierarchical')\n",
    "    ax2.set_ylim(bottom=0)    \n",
    "    \n",
    "    lines = [\n",
    "        line_1, line_2, line_high, line_hier, \n",
    "        line_std_low, line_std_diff, line_acq,\n",
    "#         line_std,\n",
    "    ]\n",
    "    \n",
    "    ax.set_title(f'Iteration {idx}')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([-16, 10])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax2.tick_params('y', colors='#555555')\n",
    "    \n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        mfbo.iteration(idx)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "ax = fig.add_axes([0,0, 1,.05])\n",
    "ax.axis('off')\n",
    "ax.legend(\n",
    "    lines, [l.get_label() for l in lines], ncol=4,\n",
    "    loc='upper center', bbox_to_anchor=(.5,1),\n",
    ")\n",
    "    \n",
    "plt.savefig(f'{plot_dir}1D_BO.png')\n",
    "plt.savefig(f'{plot_dir}1D_BO.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models on 2D functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some point styles\n",
    "red_dot = {'marker': '.', 'color': 'red'}\n",
    "blue_circle = {'marker': 'o', 'facecolors': 'none', 'color': 'blue'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D version of 1D function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the 2D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mff.row_vectorize\n",
    "def td_inv_high(xx):\n",
    "    x1, x2 = xx\n",
    "    return -(OD.high(x1) + OD.high(x2))\n",
    "\n",
    "@mff.row_vectorize\n",
    "def td_inv_low(xx):\n",
    "    x1, x2 = xx\n",
    "    return -(OD.low(x1) + OD.low(x2))\n",
    "\n",
    "\n",
    "TD_inv = mff.MultiFidelityFunction(\n",
    "    u_bound=np.array(OD.u_bound*2), l_bound=np.array(OD.l_bound*2),\n",
    "    functions=[td_inv_high, td_inv_low],\n",
    "    fidelity_names=['high', 'low'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_bound, l_bound = TD_inv.u_bound, TD_inv.l_bound\n",
    "steps = [.025, .025]\n",
    "surf_high = mlcs.createsurface(TD_inv.high, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "surf_low = mlcs.createsurface(TD_inv.low, u_bound=u_bound, l_bound=l_bound, step=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlcs.plotsurfaces([surf_high, surf_low], titles=['High fidelity (2d)', 'Low fidelity (2d)'], save_as=f'{plot_dir}2d_high_low_true_mesh.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlcs.plotsurfaces([surf_high, surf_low], as_3d=False, titles=['High fidelity (2d)', 'Low fidelity (2d)'], save_as=f'{plot_dir}2d_high_low_true.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_low = 16\n",
    "n_high = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models_and_compare(func, low, high, save_as=None):\n",
    "    archive = mlcs.CandidateArchive(ndim=2, fidelities=['high', 'low', 'high-low'])\n",
    "    archive.addcandidates(low, func.low(low), fidelity='low')\n",
    "    archive.addcandidates(high, func.high(high), fidelity='high')\n",
    "\n",
    "    mfbo = mlcs.MultiFidelityBO(func, archive, output_range=(-16, 10), schema=[1,1])\n",
    "\n",
    "    surf_high_model = mlcs.createsurface(mfbo.models['high'].predict, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "    surf_low_model = mlcs.createsurface(mfbo.models['low'].predict, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "\n",
    "    points_high = [mlcs.ScatterPoints(*archive.getcandidates(fidelity='high'), red_dot)]\n",
    "    points_low = [mlcs.ScatterPoints(*archive.getcandidates(fidelity='low'), blue_circle)]\n",
    "\n",
    "    points = [\n",
    "        points_high, points_low,\n",
    "        points_high, points_low,\n",
    "    ]\n",
    "\n",
    "    mlcs.plotsurfaces([surf_high, surf_low, surf_high_model, surf_low_model], shape=(2,2), \n",
    "                      titles=['high', 'low', 'high (hierarchical model)', 'low (model)'], all_points=points,\n",
    "                      save_as=save_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first attempt, let's simply use all 2d combinations of the coordinates originally used for the example figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_x = np.linspace(0,1,6).reshape((-1,1))\n",
    "high_x = low_x[[2,3]].reshape((-1,1))\n",
    "\n",
    "high_xy = np.array(list(product(high_x.flatten(), repeat=2)))\n",
    "low_xy =  np.array(list(product(low_x.flatten(), repeat=2)))\n",
    "\n",
    "create_models_and_compare(TD_inv, low_xy, high_xy, save_as=f'{plot_dir}2d_grid_trained_models.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20160501)\n",
    "\n",
    "low_xy = low_random_sample(ndim=2, nlow=n_low)\n",
    "high_xy = low_xy[np.random.choice(n_low, n_high, replace=False)]\n",
    "\n",
    "create_models_and_compare(TD_inv, low_xy, high_xy, save_as=f'{plot_dir}2d_random_trained_models.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With an LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20160501)\n",
    "\n",
    "low_xy = low_lhs_sample(ndim=2, nlow=n_low)\n",
    "high_xy = low_xy[np.random.choice(n_low, n_high, replace=False)]\n",
    "\n",
    "create_models_and_compare(TD_inv, low_xy, high_xy, save_as=f'{plot_dir}2d_LHS_trained_models.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE errors per sample size combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '2d_mse_tracking.npy' in os.listdir('.'):\n",
    "    mse_tracking = np.load('2d_mse_tracking.npy')\n",
    "else:\n",
    "    mse_tracking = create_mse_tracking(TD_inv, low_random_sample)\n",
    "    np.save('2d_mse_tracking.npy', mse_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = np.nanmedian(mse_tracking, axis=2)\n",
    "\n",
    "print('median')\n",
    "pprint([(f'{95+i}%-ile', np.percentile(np.nanmedian(mse_tracking, axis=2).flatten(), 95+i)) for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high_vs_low_num_samples(plot_data, '2d-high-low-samples-random', vmax=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '2d_lin_mse_tracking.npy' in os.listdir('.'):\n",
    "    lin_mse_tracking = np.load('2d_lin_mse_tracking.npy')\n",
    "else:\n",
    "    lin_mse_tracking = create_mse_tracking(TD_inv, low_lhs_sample)\n",
    "    np.save('2d_lin_mse_tracking.npy', lin_mse_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_plot_data = np.nanmedian(lin_mse_tracking, axis=2)\n",
    "\n",
    "print('median')\n",
    "pprint([(f'{95+i}%-ile', np.percentile(np.nanmedian(lin_mse_tracking, axis=2).flatten(), 95+i)) for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high_vs_low_num_samples(lin_plot_data, '2d-high-low-samples-linear', vmax=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGO on 2D functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'default' grid of initial points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_x = np.linspace(0,1,6).reshape((-1,1))\n",
    "high_x = low_x[[2,3]].reshape((-1,1))\n",
    "np.random.seed(20160501)\n",
    "\n",
    "high_xy = np.array(list(product(high_x.flatten(), repeat=2)))\n",
    "low_xy =  np.array(list(product(low_x.flatten(), repeat=2)))\n",
    "\n",
    "archive = mlcs.CandidateArchive(ndim=2, fidelities=['high', 'low', 'high-low'])\n",
    "archive.addcandidates(low_xy, TD_inv.low(low_xy), fidelity='low')\n",
    "archive.addcandidates(high_xy, TD_inv.high(high_xy), fidelity='high')\n",
    "\n",
    "mfbo = mlcs.MultiFidelityBO(TD_inv, archive, output_range=(-16, 10), schema=[2,1])\n",
    "\n",
    "fig, axes = plt.subplots(6,4, figsize=(16, 20))#, subplot_kw={'projection': '3d'})\n",
    "\n",
    "idx = 0\n",
    "for row in chunked(axes.tolist(), 2):\n",
    "    for ax1, ax2 in zip(*row):\n",
    "        \n",
    "        model_surface = mlcs.createsurface(mfbo.models['high'].predict, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "        acq = partial(mfbo.utility, gp=mfbo.models['high'], y_max=archive.max['high'])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            acq_surface = mlcs.createsurface(acq, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "\n",
    "        points = [\n",
    "            mlcs.ScatterPoints(*archive.getcandidates(fidelity='high'), red_dot),\n",
    "            mlcs.ScatterPoints(*archive.getcandidates(fidelity='low'), blue_circle),\n",
    "        ]\n",
    "        surf = mlcs.plotcmaponaxis(ax1, model_surface, title=f'high model - iteration {idx}', point_sets=points)\n",
    "        fig.colorbar(surf, ax=ax1)\n",
    "        surf = mlcs.plotcmaponaxis(ax2, acq_surface, title='acquisition function', point_sets=points)\n",
    "        fig.colorbar(surf, ax=ax2)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            mfbo.iteration(idx)\n",
    "            \n",
    "        idx += 1\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}2D_BO.pdf')\n",
    "plt.savefig(f'{plot_dir}2D_BO.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHS initial sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_low = 12\n",
    "n_high = 3\n",
    "np.random.seed(20160501)\n",
    "\n",
    "low_xy =  low_lhs_sample(ndim=2, nlow=n_low)\n",
    "high_xy = low_xy[np.random.choice(n_low, n_high, replace=False)]\n",
    "\n",
    "archive = mlcs.CandidateArchive(ndim=2, fidelities=['high', 'low', 'high-low'])\n",
    "archive.addcandidates(low_xy, TD_inv.low(low_xy), fidelity='low')\n",
    "archive.addcandidates(high_xy, TD_inv.high(high_xy), fidelity='high')\n",
    "\n",
    "mfbo = mlcs.MultiFidelityBO(TD_inv, archive, output_range=(-16, 10), schema=[2,1])\n",
    "\n",
    "fig, axes = plt.subplots(8,5, figsize=(16, 20))#, subplot_kw={'projection': '3d'})\n",
    "\n",
    "idx = 0\n",
    "for row in chunked(axes.tolist(), 2):\n",
    "    for ax1, ax2 in zip(*row):\n",
    "        \n",
    "        model_surface = mlcs.createsurface(mfbo.models['high'].predict, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "        acq = partial(mfbo.utility, gp=mfbo.models['high'], y_max=archive.max['high'])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            acq_surface = mlcs.createsurface(acq, u_bound=u_bound, l_bound=l_bound, step=steps)\n",
    "\n",
    "        points = [\n",
    "            mlcs.ScatterPoints(*archive.getcandidates(fidelity='high'), red_dot),\n",
    "            mlcs.ScatterPoints(*archive.getcandidates(fidelity='low'), blue_circle),\n",
    "        ]\n",
    "        surf = mlcs.plotcmaponaxis(ax1, model_surface, title=f'high model - iteration {idx}', point_sets=points)\n",
    "        fig.colorbar(surf, ax=ax1)\n",
    "        surf = mlcs.plotcmaponaxis(ax2, acq_surface, title='acquisition function', point_sets=points)\n",
    "        fig.colorbar(surf, ax=ax2)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            mfbo.iteration(idx)\n",
    "            \n",
    "        idx += 1\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plot_dir}2D_LHS_BO.pdf')\n",
    "plt.savefig(f'{plot_dir}2D_LHS_BO.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension to 3 fidelities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
